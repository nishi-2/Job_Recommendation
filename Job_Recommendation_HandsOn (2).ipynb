{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Basic Set up </h2>\n",
        "\n",
        "Set up environment for the recommendation system\n",
        "<br>\n",
        " -- Download and install python. <br>\n",
        " -- Download and install an IDE, like jupyter notebook, pycharm, etc. Here I will be using Google colab. </br>\n",
        " -- Start with the project :)"
      ],
      "metadata": {
        "id": "qdw1Zs5Cug0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need several libraries while progressing with the project. You can install all those using the pip install. For example, pip install pandas, pip install nltk, etc. <br>\n",
        "Google colab environment may have several pre-installed libraries. If you cannot find one, pip install it :)"
      ],
      "metadata": {
        "id": "Fg5U4KODvySb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> What is Recommendation System? </h2>\n",
        "A recommendation system is a program that helps a user discover a product or any content based on the user's interests or the user's interactions. <br/>\n",
        "Example - Ecommerce websites, social medias, OTT platforms, etc. uses recommendation system. <br/>\n",
        "Recommendation systems takes in user data and uses that data for making predictions on new products or contents which are not seen by the user but probably liked by the user. This will keep the user engaged, as they will get more of what they like. <br><br>\n",
        "There are mainly two types of recommendation system - <br>\n",
        "<ol>\n",
        "<li> Content based Recommendation system - uses the specification of the product, that is the details of the product, and recommends new products based on it. For example, if you read the book of Origin by Dan Brown and tell about it to your friend, your friend might suggest you to read the book of Inferno, by Dan Brown. This is because the genre of both the books shares similarity; also the author is the same.</li>\n",
        "<li> Collaborative filtering - This recommendation is based on the user's ratings on the products in past. It is got nothing to do with the product themselves, unlike content based. For example, a guest came to your house and gave you a chocolate. Your sister is almost like you in choices, so there is a high probability that she will like chocolates too. So the guest gives her one as well. This is collaborative.\n",
        "</ol>\n",
        "<br><br>\n",
        "<h3> How content based recommendation system works? </h3>\n",
        "Suppose you have purchased a t-shirt from Amazon. That t-shirt must be having certain decriptions. Amazon will start recommending you the tshirts which shares almost similar description, pricing, etc. Content based system works well when descriptive data is available for the products. Thus, descriptive data has to be there, which is kind of a disadvantage.\n",
        "<br><br>\n",
        "<h3> How collaborative filtering works? </h3>\n",
        "Suppose you picked up black tshirt, blue jeans. There is another person A who picked up white shirt, brown jeans, nike shoes. There is another person B who picked up black shirt, blue jeans,  puma shoes. It seems like the person B and you have almost similar in taste based on the previous pickings. So the recommender will recommend you puma shoes, as your interests overlap with that of person B. This is what collaborative is, based on user interactions.\n",
        "<br><br>\n",
        "Advantage of collaborative filtering is you do not need to have the knowledge of the product or content at all. It solely depends on user review data. Disadvantage is that you cannot make recommendation if you dont have any user review. Also, it tends to show popular products more.\n"
      ],
      "metadata": {
        "id": "Kxs1k7gAugjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> What are vectors ? </h2>\n",
        "Understanding the concept of vectors is important. We need to deal with large arrays of data in ML. These arrays are sometimes called vectors for single column of data and is called matrices for larger arrays. Let's see how to work with vectors - <br>\n",
        "Before that, I would like to brief on SIMD concept. Modern computers have the ability to take a list of numbers and apply mathematical operations on all the numbers parallely. This is known as single instruction, multiple data or SIMD. CPU can load chunk of data into memory and perform mathematical operations at once parallely."
      ],
      "metadata": {
        "id": "WcGjsMt64nfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di9SFErio4wc"
      },
      "outputs": [],
      "source": [
        "#complete the following\n",
        "\n",
        "dataset_on_rates = [4,3,4,5,2,3,4,1,5,3,2,1,3,5,4,4,2,1,2,5,2]\n",
        "new_rating = []\n",
        "for i, value in _______(dataset_on_rates):\n",
        "  print('Updating rate at {}'.format(i), end = '   ')\n",
        "  new_rating.______(i, (value * 4))\n",
        "\n",
        "print()\n",
        "print(new_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While using ML, similar mathematical operations are used over an entire array. Here, we are converting all the ratings from the scale of 5 to the scale of 20 by multiplying by 4.<br>\n",
        "Looping over the list using a for loop and doing the multiplication operation is inefficient. So instead of working in this way, we can make use of SIMD, that is we can load the full array on CPU and perform the multiplication operation at once. This creates a huge difference in speed when working with large dataset. <br><br>\n",
        "For the above reason, it is efficient to work with array libraries that can process data in parallel. This array library is the NumPy. Numpy creates array in the memory in a very efficient way and automatically parallelizes common operation on array. Thus numpy automatically takes the advantage of SIMD feature of CPU. Let's see how the code will be -"
      ],
      "metadata": {
        "id": "1AtgbF6V55FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following\n",
        "\n",
        "import ______ as np\n",
        "\n",
        "dataset_on_rates = np.array([4,3,4,5,2,3,4,1,5,3,2,1,3,5,4,4,2,1,2,5,2])\n",
        "new_ratings = _________ * 4\n",
        "print(new_ratings)"
      ],
      "metadata": {
        "id": "Wu7YC64n5u0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the operations in an array can be done in parallel following this way. This is known as vectorizing the code. We are replacing iterative codes with vector operations that can be executed in parallel."
      ],
      "metadata": {
        "id": "smAoEGTmOmAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> About our project </h3>\n",
        " This project is about job recommendation and is content based. We will go through the job description provided in the dataset and will look into the job title based on that job description. The model will learn the job description for the particular job titles. Then we will pass a job title to recommend other suitable jobs; it will show us certain job titles based on its content based learnings."
      ],
      "metadata": {
        "id": "b0EG6ydXQ_kW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting drive**"
      ],
      "metadata": {
        "id": "_p2aIPBhSSoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ci8m-zfZSXr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries**"
      ],
      "metadata": {
        "id": "Xj0uxbpWSJ1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IAXQhiATOlA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading dataset**"
      ],
      "metadata": {
        "id": "ZuM_41hbSvg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Python Codes/Global company/naukri_jobs.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cTz0xGxhSvFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring and Cleaning dataset** <br>\n",
        "We will drop the columns that we won't need. For the job recommendation system, we need the job description field mandatorily based on which we will recommend."
      ],
      "metadata": {
        "id": "i9n4PsZCS9q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following\n",
        "\n",
        "df.drop(['jobid', 'numberofpositions', 'payrate', 'postdate', 'site_name', 'uniq_id'], ________ , inplace = True)"
      ],
      "metadata": {
        "id": "2de7V8xXS83h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Zrkp9YCNSRWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "9sYEE0UpTbR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - to get the null values\n",
        "\n",
        "df.______.sum()"
      ],
      "metadata": {
        "id": "BXC50ydjTdo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are four companies with null values. Let look into those -"
      ],
      "metadata": {
        "id": "VQC0hnq1TmaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.company.isna()]"
      ],
      "metadata": {
        "id": "CLDgIAnGTjHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all are null, so we can drop these rows\n",
        "#complete the following\n",
        "\n",
        "df.dropna(subset = ['company'], ________, inplace = True)"
      ],
      "metadata": {
        "id": "uwi82f3nTqxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have some values from education field as NaN. We will replace that NaN with 'Not Given'"
      ],
      "metadata": {
        "id": "nRUOifz9T-Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['education'].isna()].head()"
      ],
      "metadata": {
        "id": "4DC3j9poTwVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - filling null values\n",
        "\n",
        "df['education']._______('Not Given', inplace = True)"
      ],
      "metadata": {
        "id": "FrAJ3Lg2UG5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, let's replace the job location NaN values and skills NaN values as Not Given."
      ],
      "metadata": {
        "id": "gRZK_cXiUTkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['joblocation_address'].fillna('Not Given', inplace = True)\n",
        "df['skills'].fillna('Not Given', inplace = True)"
      ],
      "metadata": {
        "id": "aLxq8QIXULuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single value for industry is NaN. Let's do the same"
      ],
      "metadata": {
        "id": "LuzoebbEUiCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - Fill the nan values for industry -\n",
        "\n"
      ],
      "metadata": {
        "id": "wCO94bByUbQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check for duplicates"
      ],
      "metadata": {
        "id": "jLMR3jb3UvPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.company.duplicated()]"
      ],
      "metadata": {
        "id": "BTG2A7ARUhmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 13527 rows having duplicate values for company names. We will keep the duplicates, as for every company, job location, job title, and skills vary. Let's find the number of duplicates for each company -"
      ],
      "metadata": {
        "id": "TrP_vqGEU0Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - count the number of each companies\n",
        "\n",
        "df.company.__________.rename_axis('company').reset_index(name='count')"
      ],
      "metadata": {
        "id": "5DPsCdVUUyPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will add the job description field with the job title and skills."
      ],
      "metadata": {
        "id": "Lx99RaJ-VQ4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will remove the space from the job title so that it is vectorized as one job."
      ],
      "metadata": {
        "id": "jCfcDwjqi5tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - remove spaces in between the words\n",
        "\n",
        "df['job title'] = df['jobtitle'].apply(lambda x:_____.join(________))"
      ],
      "metadata": {
        "id": "8hW-5bvshysc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['jobdescription'] = df['jobdescription'] + df['jobtitle'] + df['skills']"
      ],
      "metadata": {
        "id": "7epggTIKVLDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will clean up the job description field, keeping only words and numbers in it."
      ],
      "metadata": {
        "id": "JVFXw6zTVjZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - fill in condition for words to be alphabet or number\n",
        "\n",
        "def remove_noise(obj):\n",
        "  word_num = list([val for val in obj.split() __________________________])\n",
        "  new_string = \" \".join(word_num)\n",
        "  return (new_string)"
      ],
      "metadata": {
        "id": "UEEshsmCVgrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['jobdescription'] = df['jobdescription'].apply(remove_noise)"
      ],
      "metadata": {
        "id": "Hl7ARc6LVwrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I will turn all the words to lower case and then we will view one of the job description to look into the change."
      ],
      "metadata": {
        "id": "Uog_KsWaV44J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - convert to lowercase\n",
        "\n",
        "df['jobdescription'] = df['jobdescription'].apply(lambda x:___________)"
      ],
      "metadata": {
        "id": "7Dsl0ES2V055"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.jobdescription[2]"
      ],
      "metadata": {
        "id": "kyAcjFdWV_Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will do stemming."
      ],
      "metadata": {
        "id": "xC5aawCOo8Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - create an object of PorterStemmer\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = ____________"
      ],
      "metadata": {
        "id": "fx4TAa_po7zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - split the text and store it on stemmed list\n",
        "\n",
        "def stemming(text):\n",
        "  stemmed = []\n",
        "  for i in __________:\n",
        "    ____________(ps.stem(i))\n",
        "  \n",
        "  new_string = \" \".join(stemmed)\n",
        "  return new_string"
      ],
      "metadata": {
        "id": "pcn6nwX6pNQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['jobdescription'] = df['jobdescription'].apply(stemming)"
      ],
      "metadata": {
        "id": "uDRO1fi9plsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Start of Vectorization </h3>"
      ],
      "metadata": {
        "id": "UEjooPHpWF3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will perform text vectorization. Here we will find the similarity on the basis of job description, which means we need to calculate the similarity score of different job descriptions. But job description is all written in text, thus we need to perform vectorization on the text to calculate the similarity scores. <br><br>\n",
        "We will convert the job description into vectors. This is called text vectorization. Since we are having 21996 records, which means 21996 different job descriptions, vectorization will create 21996 vectors. So, the closest vector to our selected vector will be chosen for recommendation. <br>\n",
        "We will use the method of bag of words for text vectorization. <br><br>\n",
        "In the process of bag of words, all the words from all the job descriptions are combined. Then from that huge pool of words, we will fetch N number of words with highest frequency. Then we will take every job descriptions and will tally with the occurance of highest number of words in each. At the end, we will get a table with 21996 number of rows and N number of columns, that defines the occurance of every taken words for every movies. Now these movies has got converted to vectors in this space. The closest vectors are taken for prediction.<br><br>\n",
        "All the above procedure of text vectorization is done by the Text vectorizer from scikit learn. Removing the stop words, as it doesn't place any meaning to sentence. Vectorization is applied to the rest of the words."
      ],
      "metadata": {
        "id": "GSKpCsSgjU5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - give max features as 10000 and stop words should be english as per data\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = ______, stop_words = _______)"
      ],
      "metadata": {
        "id": "P-kwl8UIWCVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector = cv.fit_transform(df['jobdescription']).toarray()"
      ],
      "metadata": {
        "id": "TY4FhF5rnSE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector.shape"
      ],
      "metadata": {
        "id": "JMM14yiTqtKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, the text vectorization is done. Now we will calculate the distance between each of the vectors; this will be the angular distance in between the vectors (cosine angle). The vectors with the lowest distance are similar."
      ],
      "metadata": {
        "id": "xsz_xxDUnSd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the following - find the cosine similarity for the word vectors\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity = ___________________"
      ],
      "metadata": {
        "id": "2-shA1Tdkfm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, for recommendation I will pass on title. I will get the index of the title and then will get the similarity scores related to that index. Then I will sort all the similarity scores in descending order and will fetch the first 10 out of it as the most similar jobs."
      ],
      "metadata": {
        "id": "KU9eUOVhrDPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend(job):\n",
        "  job_pos = df[df['jobtitle'] == job].index[0]\n",
        "  distances = similarity[job_pos]\n",
        "  job_list = sorted(list(enumerate(distances)), reverse = True, key = lambda x:x[1])[0:10]\n",
        "  Index, Title, Company_Offering, Industry = [], [], [], []\n",
        "  count = 1\n",
        "  for j in job_list:\n",
        "    Index.append(count)\n",
        "    Title.append(df.iloc[j[0]].jobtitle)\n",
        "    Company_Offering.append(df.iloc[j[0]].company)\n",
        "    Industry.append(df.iloc[j[0]].industry)\n",
        "    count = count + 1\n",
        "  return Index, Title, Company_Offering, Industry"
      ],
      "metadata": {
        "id": "GgcfF-VUqyaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job = input('Enter job title - ')\n",
        "\n",
        "Title, Company_Offering, Industry = recommend(job)\n",
        "job_frame = pd.DataFrame({\n",
        "    'Title' : Title,\n",
        "    'Company Offering' : Company_Offering,\n",
        "    'Industry' : Industry\n",
        "  })\n",
        "print(job_frame)"
      ],
      "metadata": {
        "id": "J7wbCuhSv1aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, our job recommendation is complete :)"
      ],
      "metadata": {
        "id": "M-YjeEBOztrb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-VScyrZMyKue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}